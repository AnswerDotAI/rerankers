{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello there.\n",
    "\n",
    "_Version 0.3 of this notebook. Last updated 2024-05-17._\n",
    "\n",
    "Welcome to `rerankers`!\n",
    "\n",
    "This notebook is an introduction notebook. It doesn't solve any problem in particular, but showcases the various rerankers you can load, their arguments combinations, and the unified output format of `Results` and `RankedResults`.\n",
    "\n",
    "This is pretty much all there is to the library: a lightweight layer aiming to make it completely painless to slot in any reranker you want!\n",
    "\n",
    "\n",
    "First, let's load the `Reranker` function, and fetch our API keys from our .env file to use the API-based rerankers (Cohere, Jina, RankGPT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bclavie/miniconda3/envs/rerankers_311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# This is the only rerankers import you'll ever need for inference\n",
    "from rerankers import Reranker\n",
    "\n",
    "\n",
    "# Let's do this manually, so we don't need to install pydotenv\n",
    "import os\n",
    "\n",
    "with open(\".env\", \"r\") as file:\n",
    "    for line in file:\n",
    "        if line.strip() and not line.startswith(\"#\"):\n",
    "            key, value = line.strip().split(\"=\", 1)\n",
    "            os.environ[key] = value.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define an imaginary query, and two documents that our totally-real first-stage retrievers has pulled up from a just-as-real corpus of online movie reviews.\n",
    "\n",
    "These will serve as the documents we want to score againt the query with our various rerankers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Gone with the wind is an absolute masterpiece\"\n",
    "docs = [\n",
    "    \"Gone with the wind is a masterclass in bad storytelling.\",\n",
    "    \"Gone with the wind is an all-time classic\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty much it for the set-up phase. Let's get started with using various rerankers now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Use\n",
    "\n",
    "This section will showcase the general use of `rerankers`, as well as the output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default cross-encoder model for language en\n",
      "Warning: Model type could not be auto-mapped with the defaults list. Defaulting to TransformerRanker.\n",
      "If your model is NOT intended to be ran as a one-label cross-encoder, please reload it and specify the model_type! Otherwise, you may ignore this warning. You may specify `model_type='cross-encoder'` to suppress this warning in the future.\n",
      "Default Model: mixedbread-ai/mxbai-rerank-base-v1\n",
      "Loading TransformerRanker model mixedbread-ai/mxbai-rerank-base-v1\n",
      "No device set\n",
      "Using device mps\n",
      "No dtype set\n",
      "Using dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bclavie/miniconda3/envs/rerankers_311/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model mixedbread-ai/mxbai-rerank-base-v1\n",
      "Using device mps.\n",
      "Using dtype torch.float16.\n"
     ]
    }
   ],
   "source": [
    "ranker = Reranker(\"cross-encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooh, that's a bit moisy, maybe we want to cut down on this defaults noise? Just set `verbose` to 0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default cross-encoder model for language en\n",
      "Warning: Model type could not be auto-mapped with the defaults list. Defaulting to TransformerRanker.\n",
      "If your model is NOT intended to be ran as a one-label cross-encoder, please reload it and specify the model_type! Otherwise, you may ignore this warning. You may specify `model_type='cross-encoder'` to suppress this warning in the future.\n",
      "Loading TransformerRanker model mixedbread-ai/mxbai-rerank-base-v1\n"
     ]
    }
   ],
   "source": [
    "ranker = Reranker(\"cross-encoder\", verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! You still get a bit of information thrown at you, but most of what's going on is no longer printed.\n",
    "\n",
    "This is all you have to do to load a model, it's now ready to rank documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RankedResults(results=[Result(document=Document(text='Gone with the wind is an all-time classic', doc_id=1, metadata={}), score=3.607421875, rank=1), Result(document=Document(text='Gone with the wind is a masterclass in bad storytelling.', doc_id=0, metadata={}), score=0.685546875, rank=2)], query='Gone with the wind is an absolute masterpiece', has_scores=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = ranker.rank(query=query, docs=docs)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ Please note: the score outputted from *any* model does not have inherent meaning! It's only useful for ranking purposes, and relatively to other outputs **from the same model**. You cannot compare score between models or make assumptions based on a score seeming high or low! ⚠️\n",
    "\n",
    "`results`, just like any `rank()` output in `rerankers`, is a `RankedResults` objects. `RankedResults` has a few useful helper functions, and contains `Result` objects, which are always an atomic reranking results, containing the document's text, the document id, its 1-indexed rank as well as the score returned by the model.\n",
    "\n",
    "Not all models output scores, so RankedResults has an `has_scores` attributes that lets you know if scores are present or not. It also always contains the original `query`, for easier mapping.\n",
    "\n",
    "The most useful function of a `RankedResults` object is `top_k`, which lets you retrieve the top `k` results, as ranked by the nodel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gone with the wind is an all-time classic'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.top_k(1)[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using `rerankers` to harvest scores (perhaps for distilling knowledge into a retrieval model?), there's also a helpful function to retrieve the score for a particular doc_id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.607421875"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.get_score_by_docid(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speaking of docids, you can of course set your own! If unspecified, they'll always be integers, corresponding to the index of a given document in the input list. You can specify your own doc_ids pretty easily, by passing them to the rank() function. Please note that `doc_ids` **must** be the same length as `docs`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Result(document=Document(text='Gone with the wind is an all-time classic', doc_id='The Similar Document', metadata={}), score=3.607421875, rank=1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = ranker.rank(\n",
    "    query=query,\n",
    "    docs=docs,\n",
    "    doc_ids=[\"The Not-So Similar Document\", \"The Similar Document\"],\n",
    ")\n",
    "results.top_k(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for the basics of using a reranker, and how the `Result` and `RankedResults` objects work. The goal is simplicity, so hopefully this wasn't too much!\n",
    "\n",
    "Keep reading if you want some more information about the various models we support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Label Cross-Encoders\n",
    "Cross-encoders are probably the most common types of re-rankers.\n",
    "\n",
    "You might be familiar with bi-encoders (also called \"embeddings models\") which encode the query and the document independently to be scored later. A lot of popular modulars, like OpenAI's embeddings, Jina models, BGE-embeddings models, etc... are bi-encoder models.\n",
    "\n",
    "Cross-encoders, on the other hand, take in **both** a query and a document as input, and output a score rather than a representation. This is the basic logic behind all \"reranker\" model approaches: they're aware of the full content of both a target document and a query at inference time, which means they can take subtle interactions into account.  \n",
    "This differs from bi-encoders, which have to provide a single representation for any given document without being aware of the context in which that representation will be used.\n",
    "\n",
    "Using cross-encoders in `rerankers` is very simple, and relies on nothing more than `transformers` and `torch`.\n",
    "\n",
    "`rerankers` lets you load any model you want, but also ships with overall balanced (observe performance vs size) defaults for all model families. In the previous example, we used a default model: we just specified `cross-encoder`, which fetched the default cross-encoder model, `mixedbread-ai/mxbai-rerank-base-v1`.\n",
    "\n",
    "However, you can load any model you want! By default, `rerankers` tries to load any model given as a cross-encoder, unless it recognises it as something else, but this'll give you a warning that it's doing so, so you're better off specifying `model_type` to suppress it. Let's load a MiniLM reranker to try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TransformerRanker model cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "No device set\n",
      "Using device mps\n",
      "No dtype set\n",
      "Using dtype torch.float16\n",
      "Loaded model cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Using device mps.\n",
      "Using dtype torch.float16.\n"
     ]
    }
   ],
   "source": [
    "ranker = Reranker(\"cross-encoder/ms-marco-MiniLM-L-6-v2\", model_type=\"cross-encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it downloaded the model from the hub, and it's now ready to use, just like before (but note the different score!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Result(document=Document(text='Gone with the wind is an all-time classic', doc_id=1, metadata={}), score=4.33984375, rank=1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = ranker.rank(query=query, docs=docs)\n",
    "results.top_k(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Reranker()` does take a few optional arguments to give you more control over model loading. One of them is `lang`, which is only useful if you're using the default model. You can set it to the 2-letter ISO language code of your target language, and it'll try to load a relevant model if there is such a default. Sadly, for a lot of languages, that'll just default to a multilingual model as NLP is very English-centric. If you're aware of any language specific models, please do contribute them to the defaults list in a PR!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default cross-encoder model for language fr\n",
      "Default Model: antoinelouis/crossencoder-camembert-base-mmarcoFR\n",
      "Loading TransformerRanker model antoinelouis/crossencoder-camembert-base-mmarcoFR\n",
      "No device set\n",
      "Using device mps\n",
      "No dtype set\n",
      "Using dtype torch.float16\n",
      "Loaded model antoinelouis/crossencoder-camembert-base-mmarcoFR\n",
      "Using device mps.\n",
      "Using dtype torch.float16.\n"
     ]
    }
   ],
   "source": [
    "ranker = Reranker(\"cross-encoder\", lang=\"fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't try out all the other arguments, but these are the ones you can use for `cross-encoder` models, explained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TransformerRanker model cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "No device set\n",
      "Using device mps\n",
      "No dtype set\n",
      "Using dtype torch.float16\n",
      "Loaded model cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Using device mps.\n",
      "Using dtype torch.float16.\n"
     ]
    }
   ],
   "source": [
    "ranker = Reranker(\n",
    "    \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    model_type=\"cross-encoder\",\n",
    "    verbose=1,  # How verbose the reranker will be. Defaults to 1, setting it to 0 will suppress most messages.\n",
    "    dtype=None,  # Which dtype the model should use. If None will figure out if your platform + model combo supports fp16 and use it if so, other fp32.\n",
    "    device=None,  # Which device the model should use. If None will figure out what the most powerful supported platform available is (cuda > mps > cpu)\n",
    "    batch_size=16,  # The batch size the model will use. Defaults to 16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all there is to this part! The defaults are generally pretty robust, but feel free to tweak them to your heart's content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API-based models\n",
    "\n",
    "There is an increasing amount of providers training powerful reranking models, and making them available via API calls. `rerankers` aims to support all the major ones, and make it as easy as possible to use them. Currently, we support [Jina.ai reranker](https://jina.ai/reranker/) and [Cohere Rerank](https://cohere.com/rerank).\n",
    "\n",
    "Loading and using them is extremely easy. All you need is an API key, and you're ready to go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-updated model_name to jina-reranker-v1-base-en for API provider jina\n",
      "Loading APIRanker model jina-reranker-v1-base-en\n",
      "{'model': 'jina-reranker-v1-base-en', 'usage': {'total_tokens': 36, 'prompt_tokens': 36}, 'results': [{'index': 0, 'document': {'text': 'Gone with the wind is a masterclass in bad storytelling.'}, 'relevance_score': 0.7841538190841675}, {'index': 1, 'document': {'text': 'Gone with the wind is an all-time classic'}, 'relevance_score': 0.6780072450637817}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Result(document=Document(text='Gone with the wind is a masterclass in bad storytelling.', doc_id=0, metadata={}), score=0.7841538190841675, rank=1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jina\n",
    "ranker = Reranker(\"jina\", api_key=os.environ[\"JINA_API_KEY\"])\n",
    "results = ranker.rank(query=query, docs=docs)\n",
    "results.top_k(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-updated model_name to rerank-english-v3.0 for API provider cohere\n",
      "Loading APIRanker model rerank-english-v3.0\n",
      "{'id': 'f8178696-a98f-41db-acee-144bfa5b516e', 'results': [{'document': {'text': 'Gone with the wind is a masterclass in bad storytelling.'}, 'index': 0, 'relevance_score': 0.9961606}, {'document': {'text': 'Gone with the wind is an all-time classic'}, 'index': 1, 'relevance_score': 0.98415464}], 'meta': {'api_version': {'version': '1'}, 'billed_units': {'search_units': 1}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Result(document=Document(text='Gone with the wind is a masterclass in bad storytelling.', doc_id=0, metadata={}), score=0.9961606, rank=1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cohere\n",
    "ranker = Reranker(\"cohere\", api_key=os.environ[\"COHERE_API_KEY\"])\n",
    "results = ranker.rank(query=query, docs=docs)\n",
    "results.top_k(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohere also supports two very-nice features: \n",
    "- It provides a multilingual version of its reranker.\n",
    "- It allows you to fine-tune models, to be served by their API.\n",
    "\n",
    "We support both of these features. To use the multilingual version of cohere's reranker, simply explicitely pass a `lang` argument, just like you would to load the default for a non-English language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-updated model_name to rerank-english-v3.0 for API provider cohere\n",
      "Loading APIRanker model rerank-english-v3.0\n",
      "{'id': '310200e1-122a-4563-98b9-a0a9a92fd69d', 'results': [{'document': {'text': 'The silmarillion is a prequel to the Lord of The Rings...'}, 'index': 1, 'relevance_score': 0.029256709}, {'document': {'text': 'Green Lantern uses a powerful ring to rule over his planet...'}, 'index': 2, 'relevance_score': 0.0014721896}, {'document': {'text': 'Dune is an incredibly confusing masterpiece in worldbuilding...'}, 'index': 0, 'relevance_score': 6.4522144e-05}], 'meta': {'api_version': {'version': '1'}, 'billed_units': {'search_units': 1}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RankedResults(results=[Result(document=Document(text='The silmarillion is a prequel to the Lord of The Rings...', doc_id=1, metadata={}), score=0.029256709, rank=1), Result(document=Document(text='Green Lantern uses a powerful ring to rule over his planet...', doc_id=2, metadata={}), score=0.0014721896, rank=2), Result(document=Document(text='Dune is an incredibly confusing masterpiece in worldbuilding...', doc_id=0, metadata={}), score=6.4522144e-05, rank=3)], query='Tell me about lord of the rings', has_scores=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cohere\n",
    "ranker = Reranker(\"cohere\", lang=\"en\", api_key=os.environ[\"COHERE_API_KEY\"])\n",
    "ranker.rank(\n",
    "    query=\"Tell me about lord of the rings\",\n",
    "    docs=[\n",
    "        \"Dune is an incredibly confusing masterpiece in worldbuilding...\",\n",
    "        \"The silmarillion is a prequel to the Lord of The Rings...\",\n",
    "        \"Green Lantern uses a powerful ring to rule over his planet...\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a pre-trained model, it's slightly different. It won't necessarily have `cohere` in its model name, so you need to let rerankers know you're trying to load a cohere model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Model type could not be auto-mapped with the defaults list. Defaulting to TransformerRanker.\n",
      "If your model is NOT intended to be ran as a one-label cross-encoder, please reload it and specify the model_type! Otherwise, you may ignore this warning. You may specify `model_type='cross-encoder'` to suppress this warning in the future.\n",
      "Loading TransformerRanker model my-finetuned-model-name\n"
     ]
    }
   ],
   "source": [
    "# wrap in a try/except, as we don't have a fine-tuned model to use for this example!\n",
    "try:\n",
    "    ranker = Reranker(\n",
    "        \"my-finetuned-model-name\",\n",
    "        api_provider=\"cohere\",\n",
    "        api_key=os.environ[\"COHERE_API_KEY\"],\n",
    "    )\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5-Based Rerankers\n",
    "\n",
    "T5-based rerankers leverage T5, a SequenceToSequence Encoder-Decoder language model, to rank documents. They generally do so by querying the model, and constraining its prediction to two tokens, one representing relevance and the other irrelevance. Those logits being then usable as relative relevance scores, similarly to cross-encoder based rerankers.\n",
    "\n",
    "They have been popular in the Information Retrieval litterature, and perform quite well on some tasks.Load a t5-based model using the same `Reranker()` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default t5 model for language en\n",
      "Default Model: unicamp-dl/InRanker-base\n",
      "Loading T5Ranker model unicamp-dl/InRanker-base\n",
      "No device set\n",
      "Using device cpu\n",
      "No dtype set\n",
      "Device set to `cpu`, setting dtype to `float32`\n",
      "Using dtype torch.float32\n",
      "Loading model unicamp-dl/InRanker-base, this might take a while...\n",
      "Using device cpu.\n",
      "Using dtype torch.float32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 true token set to ▁true\n",
      "T5 false token set to ▁false\n",
      "Returning normalised scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring...:   0%|          | 0/1 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Scoring...: 100%|██████████| 1/1 [00:00<00:00,  2.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Result(document=Document(text='Gone with the wind is a masterclass in bad storytelling.', doc_id=0, metadata={}), score=0.9964194297790527, rank=1),\n",
       " Result(document=Document(text='Gone with the wind is an all-time classic', doc_id=1, metadata={}), score=0.9739130139350891, rank=2)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker = Reranker(\"t5\")\n",
    "results = ranker.rank(query=query, docs=docs)\n",
    "results.top_k(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's quite a lot going on with T5 models' initialisation, so you might want to pass `verbose=0` if you don't care.\n",
    "Of course, you can always use `model_type='t5'` to load any non-default t5 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading T5Ranker model unicamp-dl/ptt5-base-pt-msmarco-10k-v2\n"
     ]
    }
   ],
   "source": [
    "ranker = Reranker(\"unicamp-dl/ptt5-base-pt-msmarco-10k-v2\", model_type=\"t5\", verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full argument for T5 models are in the same vein as for ColBERT: they follow the transformers ones, along with a bunch of model-specific ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default t5 model for language en\n",
      "Default Model: unicamp-dl/InRanker-base\n",
      "Loading T5Ranker model unicamp-dl/InRanker-base\n",
      "No device set\n",
      "Using device cpu\n",
      "No dtype set\n",
      "Device set to `cpu`, setting dtype to `float32`\n",
      "Using dtype torch.float32\n",
      "Loading model unicamp-dl/InRanker-base, this might take a while...\n",
      "Using device cpu.\n",
      "Using dtype torch.float32.\n",
      "T5 true token set to ▁true\n",
      "T5 false token set to ▁false\n",
      "Returning normalised scores...\n"
     ]
    }
   ],
   "source": [
    "ranker = Reranker(\n",
    "    \"t5\",\n",
    "    model_type=\"t5\",\n",
    "    verbose=1,  # How verbose the reranker will be. Defaults to 1, setting it to 0 will suppress most messages.\n",
    "    dtype=None,  # Which dtype the model should use. If None will figure out if your platform + model combo supports fp16 and use it if so, other fp32.\n",
    "    device=None,  # Which device the model should use. If None will figure out what the most powerful supported platform available is (cuda > mps > cpu)\n",
    "    batch_size=16,  # The batch size the model will use. Defaults to 16\n",
    "    token_false=\"auto\",  # The output token corresponding to non-relevance.\n",
    "    token_true=\"auto\",  # The output token corresponding to relevance.\n",
    "    return_logits=False,  # Whether to return a normalised score or the raw logit for `token_true`.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`token_false` and `token_true` are very important! Leaving them to `auto` will use our mapping of the ones used by the most popular t5 rerankers, or default to the most frequently used ones if not present. If you're using a custom T5 reranker, you probably want to specify exactly which token your model was trained with, but you can safely ignore it for off-the-shelves models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RankGPT\n",
    "\n",
    "RankGPT is a new reranking approach, which leverages LLMs to perform zero-shot reranking. The idea is pretty simple: give a powerful LLM your query and your documents, and have it perform listwise reranking: that is, compare the documents to each other and create a relevance ranking. It works surprisingly well in a variety of zero-shot contexts.\n",
    "\n",
    "The initial approach uses OpenAI's GPT-3.5 and GPT-4, though there are now more work in the area, including RankZephyr, a specifically finetuned 7B LLM.\n",
    "We currently only support the original RankGPT implementation, although you can use it with any LLM provider thanks to LiteLLM.\n",
    "\n",
    "Loading a RankGPT ranker is pretty similar to loading any other model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default rankgpt model for language en\n",
      "Default Model: gpt-4-turbo-preview\n",
      "Loading RankGPTRanker model gpt-4-turbo-preview\n"
     ]
    }
   ],
   "source": [
    "from rerankers import Reranker\n",
    "\n",
    "ranker = Reranker(\"rankgpt\", api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default \"rankgpt\" model uses `gpt-4-turbo-preview`. You can also load `rankgpt3` to use gpt-3.5, or `rankgpt4` to use GPT-4 (non-Turbo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default rankgpt3 model for language en\n",
      "Default Model: gpt-3.5-turbo\n",
      "Loading RankGPTRanker model gpt-3.5-turbo\n",
      "Querying model gpt-3.5-turbo with via LiteLLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Result(document=Document(text='Gone with the wind is a masterclass in bad storytelling.', doc_id=0, metadata={}), score=None, rank=1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rerankers import Reranker\n",
    "\n",
    "ranker = Reranker(\"rankgpt3\", api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "results = ranker.rank(query=query, docs=docs)\n",
    "results.top_k(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there's no score provided for RankGPT models, as it never outputs one and all the reranking is done inside the \"black box\".\n",
    "\n",
    "Finally, we use LiteLLM as the backend, in order to support different providers. You can check out [their documentation](https://litellm.vercel.app/) to see how to use different LLM providers. All you need to do, in that case, is to specify `model_type=\"rankgpt\"`. For instance, if you wanted to use RankGPT with an OpenAI Azure deployment, you'd do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RankGPTRanker model azure/my-azure-gpt-deployment\n"
     ]
    }
   ],
   "source": [
    "# LiteLLM uses env variables\n",
    "import os\n",
    "\n",
    "os.environ[\"AZURE_API_KEY\"] = \"\"\n",
    "os.environ[\"AZURE_API_BASE\"] = \"\"\n",
    "os.environ[\"AZURE_API_VERSION\"] = \"\"\n",
    "deployment_name = \"my-azure-gpt-deployment\"\n",
    "\n",
    "# Just like Cohere's finetuned rankers above -- we try/except this as we're not actually running an Azure OpenAI model in this example!\n",
    "try:\n",
    "    ranker = Reranker(\n",
    "        f\"azure/{deployment_name}\",\n",
    "        model_type=\"rankgpt\",\n",
    "        api_key=os.environ[\"AZURE_API_KEY\"],\n",
    "    )\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RankLLM\n",
    "\n",
    "RankLLM is a refinement on the RankGPT approach, by [Jimmy Lin's lab at the University of Waterloo](http://castorini.io).\n",
    "\n",
    "It introduces a safer, more refined codebase for RankGPT calls, as well as the possibility to use non-GPT models, with very strong performance from just 7B models such as RankVicuna and RankZephyr.\n",
    "\n",
    "It effectively functions the same as RankGPT, with the added possibility (CURRENTLY UNTESTED) to use local models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default rankllm model for language en\n",
      "Default Model: gpt-4o\n",
      "Loading RankGPTRanker model gpt-4o\n",
      "Querying model gpt-4o with via LiteLLM...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RankedResults(results=[Result(document=Document(text='Gone with the wind is a masterclass in bad storytelling.', doc_id=0, metadata={}), score=None, rank=1), Result(document=Document(text='Gone with the wind is an all-time classic', doc_id=1, metadata={}), score=None, rank=2)], query='Gone with the wind is an absolute masterpiece', has_scores=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rerankers import Reranker\n",
    "\n",
    "ranker = Reranker(\"rankllm\", api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "ranker.rank(query=query, docs=docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, passing in just the name of a `GPT` model, such as `gpt-4-turbo`, to your `Reranker()` initialisation will initialise RankGPT. This will change in version 0.0.5, at which point it'll default to RankLLM. If you'd like to keep a stable behaviour accross versions, or use RankLLM with `GPT` models already, just pass in the `model_type` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RankLLMRanker model gpt-4-turbo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RankedResults(results=[Result(document=Document(text='Gone with the wind is an all-time classic', doc_id=1, metadata={}), score=None, rank=0), Result(document=Document(text='Gone with the wind is a masterclass in bad storytelling.', doc_id=0, metadata={}), score=None, rank=1)], query='Gone with the wind is an absolute masterpiece', has_scores=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker = Reranker(\n",
    "    \"gpt-4-turbo\", model_type=\"rankllm\", api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "ranker.rank(query=query, docs=docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ColBERT Rerankers\n",
    "\n",
    "This one is a bit of a tricky case: it leverages ColBERT and its late-interaction approach to re-rank documents. ColBERT, however, is actually closer to a bi-encoder in spirit: it encodes documents without having any knowledge of the query (and vice-versa), and scores them at a later time. However, it's a very powerful retrieval model, and can be a very strong zero-shot reranker in some settings.\n",
    "\n",
    "Loading a ColBERT model is similar to loading any other model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bclavie/miniconda3/envs/rerankers_311/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default colbert model for language en\n",
      "Default Model: colbert-ir/colbertv2.0\n",
      "Loading ColBERTRanker model colbert-ir/colbertv2.0\n",
      "No device set\n",
      "Using device mps\n",
      "No dtype set\n",
      "Using dtype torch.float16\n",
      "Loading model colbert-ir/colbertv2.0, this might take a while...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Result(document=Document(text='Gone with the wind is an all-time classic', doc_id=1, metadata={}), score=0.9578188061714172, rank=1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rerankers import Reranker\n",
    "\n",
    "ranker = Reranker(\"colbert\")\n",
    "results = ranker.rank(query=query, docs=docs)\n",
    "results.top_k(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can of course load any non-default ColBERT model via the `model_type` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ColBERTRanker model antoinelouis/colbertv2-camembert-L4-mmarcoFR\n",
      "No device set\n",
      "Using device mps\n",
      "No dtype set\n",
      "Using dtype torch.float16\n",
      "Loading model antoinelouis/colbertv2-camembert-L4-mmarcoFR, this might take a while...\n"
     ]
    }
   ],
   "source": [
    "ranker = Reranker(\"antoinelouis/colbertv2-camembert-L4-mmarcoFR\", model_type=\"colbert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full argument for ColBERT models are pretty similar to other transformers-based models, except for a few ColBERT-specific argument you'd only want to modify for custom models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default colbert model for language en\n",
      "Default Model: colbert-ir/colbertv2.0\n",
      "Loading ColBERTRanker model colbert-ir/colbertv2.0\n",
      "No device set\n",
      "Using device mps\n",
      "No dtype set\n",
      "Using dtype torch.float16\n",
      "Loading model colbert-ir/colbertv2.0, this might take a while...\n"
     ]
    }
   ],
   "source": [
    "ranker = Reranker(\n",
    "    \"colbert\",\n",
    "    model_type=\"colbert\",\n",
    "    verbose=1,  # How verbose the reranker will be. Defaults to 1, setting it to 0 will suppress most messages.\n",
    "    dtype=None,  # Which dtype the model should use. If None will figure out if your platform + model combo supports fp16 and use it if so, other fp32.\n",
    "    device=None,  # Which device the model should use. If None will figure out what the most powerful supported platform available is (cuda > mps > cpu)\n",
    "    batch_size=16,  # The batch size the model will use. Defaults to 16\n",
    "    query_token=\"[unused0]\",  # A ColBERT-specific argument. The token that your model prepends to queries.\n",
    "    document_token=\"[unused1]\",  # A ColBERT-specific argument. The token that your model prepends to documents.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FlashRank Rerankers\n",
    "\n",
    "[FlashRank](https://github.com/PrithivirajDamodaran/FlashRank) is not a new approach, but an optimisation one. It's a library maintained by [Prithiviraj Damodaran](https://github.com/PrithivirajDamodaran), which provides ONNX weights optimised for CPU inference for various common reranking methods.\n",
    "\n",
    "Rerankers wraps FlashRank, so you can load any flashrank model the usual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default flashrank model for language en\n",
      "Default Model: ms-marco-MiniLM-L-12-v2\n",
      "Loading FlashRankRanker model ms-marco-MiniLM-L-12-v2\n",
      "Loading model FlashRank model ms-marco-MiniLM-L-12-v2...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Result(document=Document(text='Gone with the wind is a masterclass in bad storytelling.', doc_id=0, metadata={}), score=0.9945825934410095, rank=1)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker = Reranker(\"flashrank\")  # Defaults to MiniLM-L12-v2\n",
    "results = ranker.rank(query=query, docs=docs)\n",
    "results.top_k(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also load any model supported by the flashrank library, with the usual explicit `model_type` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flashrank.Ranker:Downloading ms-marco-TinyBERT-L-2-v2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FlashRankRanker model ms-marco-TinyBERT-L-2-v2\n",
      "Loading model FlashRank model ms-marco-TinyBERT-L-2-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ms-marco-TinyBERT-L-2-v2.zip: 100%|██████████| 3.26M/3.26M [00:00<00:00, 7.75MiB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RankedResults(results=[Result(document=Document(text='Gone with the wind is a masterclass in bad storytelling.', doc_id=0, metadata={}), score=0.9263709783554077, rank=1), Result(document=Document(text='Gone with the wind is an all-time classic', doc_id=1, metadata={}), score=0.8937801122665405, rank=2)], query='Gone with the wind is an absolute masterpiece', has_scores=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker = Reranker(\"ms-marco-TinyBERT-L-2-v2\", model_type=\"flashrank\")\n",
    "results = ranker.rank(query=query, docs=docs)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  That's all folks! Thanks for checking the full overview."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rerankers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
